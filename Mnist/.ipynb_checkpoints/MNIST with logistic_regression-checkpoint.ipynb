{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(root = 'traindata/', download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 60,000 images that we'll use to train the model. There is also an additional test set of 10,000 images used for evaluating models and reporting metrics in papers and reports. We can create the test dataset using the MNIST class by passing train=False to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root = 'testdata/',train = False, download = True)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7F4A776D9550>, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pair, consisting of a 28x28 px image and a label. The image is an object of the class PIL.Image.Image, which is a part of the Python imaging library Pillow. We can view the image within Jupyter using matplotlib, the de-facto plotting and graphing library for data science in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADeZJREFUeJzt3WGMVPW5x/Hf45b6AngBElbc0tIiGhtj7GVDbgJpWlubrSHBRqIlJm4jdvuixNt41ateTU1uGqFpK7wwTbYRC6YFNKKSpmm1RGtrKnHZqCi0Fck2hSxsAROs0SD43BdzaFfc+Z9h5sycs/t8P8lmZ84z55wnJ/vbMzP/M/M3dxeAeM4ruwEA5SD8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC+kQnd2ZmXE4ItJm7WyOPa+nMb2Z9ZvYXM9tvZne1si0AnWXNXttvZl2S/irpakkHJb0saZW7702sw5kfaLNOnPmXSNrv7gfc/aSkrZJWtLA9AB3USvh7JP193P2D2bKPMLMBMxsys6EW9gWgYG1/w8/dByUNSjztB6qklTP/IUnzx93/VLYMwCTQSvhflrTIzD5rZp+U9E1JO4ppC0C7Nf20391PmdkaSb+V1CVpo7u/UVhnANqq6aG+pnbGa36g7TpykQ+AyYvwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoJqeoluSzGxE0juSTks65e69RTSF6rj44ouT9VtvvTVZX7NmTd2aWXoy2VOnTiXrt9xyS7K+ZcuWurWTJ08m142gpfBnvuzuRwvYDoAO4mk/EFSr4XdJz5jZbjMbKKIhAJ3R6tP+Ze5+yMzmSnrWzP7s7i+Mf0D2T4F/DEDFtHTmd/dD2e8xSU9KWjLBYwbdvZc3A4FqaTr8ZjbdzGaeuS3pa5JeL6oxAO3VytP+bklPZsM1n5D0S3f/TSFdAWg7c/fO7cysczuDJKmrqytZv+mmm5L1devWJetz5sw5557OGBsbS9bnzp3b9LYladGiRXVrb731VkvbrjJ3T19AkWGoDwiK8ANBEX4gKMIPBEX4gaAIPxAUQ31TwKpVq+rWFi9enFz3tttua2nfTz31VLL+0EMP1a3lDbdt3bo1WV+y5GMXlH7E888/X7d21VVXJdedzBjqA5BE+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc4/CaS+/lqSNmzYULeW9/XYx44dS9b7+vqS9eHh4WS9lb+vGTNmJOsnTpxoet9Lly5NrvvSSy8l61XGOD+AJMIPBEX4gaAIPxAU4QeCIvxAUIQfCKqIWXrRorzx7Lxx/tRY/rvvvptcd/ny5cn67t27k/V2yptGe9++fcn6ZZddVmQ7Uw5nfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IKnec38w2SlouaczdL8+WzZa0TdICSSOSrnf3t9vX5tQ2c+bMZP2SSy5petvr169P1nft2tX0ttstb5x/z549yTrj/GmNnPl/Lunsb3S4S9JOd18kaWd2H8Akkht+d39B0vGzFq+QtCm7vUnStQX3BaDNmn3N3+3uo9ntw5K6C+oHQIe0fG2/u3vqu/nMbEDSQKv7AVCsZs/8R8xsniRlv8fqPdDdB9291917m9wXgDZoNvw7JPVnt/slPV1MOwA6JTf8ZrZF0p8kXWpmB81staS1kq42szclfTW7D2ASyX3N7+71Jn//SsG9hHXBBRe0tH7qM/uPPPJIS9vG1MUVfkBQhB8IivADQRF+ICjCDwRF+IGg+OruCli5cmVL6z/22GN1awcOHGhp25i6OPMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM83dA3kd2V69e3dL2h4aGWlq/qs4///xkfenSpR3qZGrizA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHO3wGXXnppst7T09PS9o8fP3se1amhq6srWc87bu+//37d2nvvvddUT1MJZ34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCCp3nN/MNkpaLmnM3S/Plt0v6duS/pE97B53/3W7mkTajh07ym6hkvbv31+39uqrr3awk2pq5Mz/c0l9Eyx/0N2vzH4IPjDJ5Ibf3V+QNDUvIQMCa+U1/xoze83MNprZrMI6AtARzYb/p5IWSrpS0qikH9d7oJkNmNmQmU3NL5oDJqmmwu/uR9z9tLt/KOlnkpYkHjvo7r3u3ttskwCK11T4zWzeuLvfkPR6Me0A6JRGhvq2SPqSpDlmdlDS9yV9ycyulOSSRiR9p409AmiD3PC7+6oJFj/chl6Aj+jv729p/XXr1hXUydTEFX5AUIQfCIrwA0ERfiAowg8ERfiBoMzdO7czs87trEKmTZuWrO/duzdZX7hwYbI+ffr0urUqf0X1hRdemKwPDw+3tP5FF11Ut3b48OHkupOZu1sjj+PMDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMUV3B3zwwQfJ+unTpzvUSbUsW7YsWc8bx887bp28hmUy4swPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzj8F9PT01K2lpqnuhLlz59at3Xvvvcl188bxV69enawfOXIkWY+OMz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBJU7zm9m8yVtltQtySUNuvsGM5staZukBZJGJF3v7m+3r9Wpa9u2bcn6fffdl6yvXLmybm3t2rVN9dSorq6uZP3OO++sW7viiiuS646OjibrmzdvTtaR1siZ/5Sk/3b3z0v6T0nfNbPPS7pL0k53XyRpZ3YfwCSRG353H3X34ez2O5L2SeqRtELSpuxhmyRd264mARTvnF7zm9kCSV+QtEtSt7ufeV52WLWXBQAmiYav7TezGZKekPQ9dz9h9u/pwNzd683DZ2YDkgZabRRAsRo685vZNNWC/wt3354tPmJm87L6PEljE63r7oPu3uvuvUU0DKAYueG32in+YUn73P0n40o7JPVnt/slPV18ewDaJXeKbjNbJukPkvZI+jBbfI9qr/sfk/RpSX9TbajveM62+C7lCVx33XXJ+uOPP56sj4yM1K0tXrw4ue7bb7c2OnvjjTcm648++mjd2vHjyT8X9fX1JetDQ0PJelSNTtGd+5rf3f8oqd7GvnIuTQGoDq7wA4Ii/EBQhB8IivADQRF+ICjCDwTFV3dXwHPPPZesHzt2LFlfsGBB3dodd9yRXPfBBx9M1m+++eZkPfWR3Tzr169P1hnHby/O/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVO7n+QvdGZ/nb0pvb/pLkF588cW6tWnTpiXXPXr0aLI+e/bsZP2889Lnj+3bt9et3XDDDcl186boxsQa/Tw/Z34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIpx/ing9ttvr1u7++67k+vOmjWrpX0/8MADyXrq+wLyrjFAcxjnB5BE+IGgCD8QFOEHgiL8QFCEHwiK8ANB5Y7zm9l8SZsldUtySYPuvsHM7pf0bUn/yB56j7v/OmdbjPMDbdboOH8j4Z8naZ67D5vZTEm7JV0r6XpJ/3T3HzXaFOEH2q/R8OfO2OPuo5JGs9vvmNk+ST2ttQegbOf0mt/MFkj6gqRd2aI1ZvaamW00swmvEzWzATMbMjPmXgIqpOFr+81shqTfS/qBu283s25JR1V7H+D/VHtpkJzYjaf9QPsV9ppfksxsmqRfSfqtu/9kgvoCSb9y98tztkP4gTYr7IM9ZmaSHpa0b3zwszcCz/iGpNfPtUkA5Wnk3f5lkv4gaY+kD7PF90haJelK1Z72j0j6TvbmYGpbnPmBNiv0aX9RCD/QfnyeH0AS4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKjcL/As2FFJfxt3f062rIqq2ltV+5LorVlF9vaZRh/Y0c/zf2znZkPu3ltaAwlV7a2qfUn01qyyeuNpPxAU4QeCKjv8gyXvP6WqvVW1L4nemlVKb6W+5gdQnrLP/ABKUkr4zazPzP5iZvvN7K4yeqjHzEbMbI+ZvVL2FGPZNGhjZvb6uGWzzexZM3sz+z3hNGkl9Xa/mR3Kjt0rZnZNSb3NN7PnzGyvmb1hZv+VLS/12CX6KuW4dfxpv5l1SfqrpKslHZT0sqRV7r63o43UYWYjknrdvfQxYTP7oqR/Stp8ZjYkM/uhpOPuvjb7xznL3f+nIr3dr3OcublNvdWbWfpbKvHYFTnjdRHKOPMvkbTf3Q+4+0lJWyWtKKGPynP3FyQdP2vxCkmbstubVPvj6bg6vVWCu4+6+3B2+x1JZ2aWLvXYJfoqRRnh75H093H3D6paU367pGfMbLeZDZTdzAS6x82MdFhSd5nNTCB35uZOOmtm6cocu2ZmvC4ab/h93DJ3/w9JX5f03ezpbSV57TVblYZrfippoWrTuI1K+nGZzWQzSz8h6XvufmJ8rcxjN0FfpRy3MsJ/SNL8cfc/lS2rBHc/lP0ek/Skai9TquTImUlSs99jJffzL+5+xN1Pu/uHkn6mEo9dNrP0E5J+4e7bs8WlH7uJ+irruJUR/pclLTKzz5rZJyV9U9KOEvr4GDObnr0RIzObLulrqt7swzsk9We3+yU9XWIvH1GVmZvrzSytko9d5Wa8dveO/0i6RrV3/N+S9L9l9FCnr89JejX7eaPs3iRtUe1p4AeqvTeyWtIFknZKelPS7yTNrlBvj6o2m/NrqgVtXkm9LVPtKf1rkl7Jfq4p+9gl+irluHGFHxAUb/gBQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjq/wFv9n1LpdtZwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a776d9f28>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image, label = dataset[1000]\n",
    "plt.imshow(image, cmap = 'gray')\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch datasets allow us to specify one or more transformation functions that are applied to the images as they are loaded. The torchvision.transforms module contains many such predefined functions. We'll use the ToTensor transform to convert images into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms \n",
    "dataset = MNIST(root = 'traindata/', train = True, transform = transforms.ToTensor())\n",
    "\n",
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now converted to a 1x28x28 tensor. The first dimension tracks color channels. The second and third dimensions represent pixels along the height and width of the image, respectively. Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in which case there are three channels: red, green, and blue (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[0, 10:15, 10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a7014d208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a70098a58>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0, 10:15, 10:15], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets\n",
    "\n",
    "Training set - used to train the model, i.e., compute the loss and adjust the model's weights using gradient descent.\n",
    "\n",
    "Validation set - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model.\n",
    "\n",
    "Test set - used to compare different models or approaches and report the model's final accuracy.\n",
    "In the MNIST dataset, there are 60,000 training images and 10,000 test images. The test set is standardized so that different researchers can report their models' results against the same collection of images.\n",
    "\n",
    "Since there's no predefined validation set, we must manually split the 60,000 images into training and validation datasets. Let's set aside 10,000 randomly chosen images for validation. We can do this using the random_spilt method from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's essential to choose a random sample for creating a validation set. Training data is often sorted by the target labels, i.e., images of 0s, followed by 1s, followed by 2s, etc. If we create a validation set using the last 20% of images, it would only consist of 8s and 9s. In contrast, the training set would contain no 8s or 9s. Such a training-validation would make it impossible to train a useful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader #with dataloader check tensordataset which is used to combine data and labels.\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, shuffle = True, batch_size = 128)\n",
    "val_loader = DataLoader(val_ds, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0153,  0.0157, -0.0338,  ...,  0.0144, -0.0087, -0.0035],\n",
      "        [ 0.0186,  0.0141,  0.0015,  ...,  0.0301,  0.0066,  0.0048],\n",
      "        [-0.0218, -0.0104,  0.0109,  ...,  0.0348, -0.0167, -0.0056],\n",
      "        ...,\n",
      "        [ 0.0171, -0.0070, -0.0250,  ...,  0.0203,  0.0053, -0.0192],\n",
      "        [ 0.0119, -0.0180, -0.0237,  ..., -0.0257, -0.0318,  0.0144],\n",
      "        [ 0.0117, -0.0173,  0.0185,  ...,  0.0155, -0.0147, -0.0189]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0120, -0.0345,  0.0187, -0.0085, -0.0356,  0.0217, -0.0114,  0.0238,\n",
      "        -0.0275,  0.0213], requires_grad=True)]\n",
      "weight shape: torch.Size([10, 784])\n",
      "bias shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "print(list(model.parameters()))\n",
    "print(\"weight shape:\", model.weight.shape)\n",
    "print(\"bias shape:\", model.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights shape: torch.Size([10, 784])\n",
      "model bias shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"model weights shape:\",model.linear.weight.shape)\n",
    "print(\"model bias shape:\",model.linear.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([128, 10])\n",
      "Sample output: tensor([[ 0.0329,  0.2100, -0.0568,  0.0221, -0.3458,  0.0491, -0.0395, -0.0555,\n",
      "         -0.3993,  0.0115]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model.forward(images)\n",
    "    break\n",
    "\n",
    "print('Output shape:', outputs.shape)\n",
    "print('Sample output:', outputs[:1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities: tensor([[0.1079, 0.1287, 0.0986, 0.1067, 0.0739, 0.1096, 0.1003, 0.0987, 0.0700,\n",
      "         0.1056]], grad_fn=<SliceBackward>)\n",
      "Sum: 0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(outputs, dim = 1)\n",
    "print(\"Sample probabilities:\", probs[:1])\n",
    "\n",
    "print(\"Sum:\", torch.sum(probs[1]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: tensor([1, 3, 6, 6, 0, 6, 2, 3, 9, 3, 6, 3, 6, 9, 7, 6, 7, 9, 3, 1, 9, 2, 6, 3,\n",
      "        1, 5, 2, 3, 3, 6, 2, 2, 5, 9, 1, 2, 5, 6, 6, 5, 2, 2, 5, 2, 6, 3, 1, 5,\n",
      "        6, 6, 1, 1, 2, 6, 1, 5, 5, 3, 3, 3, 2, 5, 2, 6, 6, 3, 2, 3, 3, 1, 3, 6,\n",
      "        3, 9, 6, 9, 7, 1, 2, 0, 6, 1, 3, 3, 6, 7, 1, 6, 6, 5, 2, 4, 5, 6, 5, 2,\n",
      "        3, 5, 5, 1, 0, 2, 5, 6, 6, 6, 3, 1, 7, 2, 1, 9, 9, 1, 3, 1, 1, 2, 3, 0,\n",
      "        3, 3, 9, 3, 6, 1, 5, 6])\n",
      "labels: tensor([4, 1, 8, 4, 0, 9, 8, 1, 8, 1, 3, 8, 3, 6, 7, 2, 0, 0, 1, 7, 2, 7, 9, 2,\n",
      "        9, 6, 5, 1, 0, 1, 5, 7, 3, 8, 8, 0, 1, 9, 5, 4, 0, 9, 3, 9, 8, 8, 5, 5,\n",
      "        5, 3, 3, 1, 8, 9, 9, 3, 1, 7, 6, 2, 7, 6, 9, 9, 3, 6, 7, 7, 2, 9, 1, 5,\n",
      "        1, 8, 2, 8, 6, 1, 5, 0, 0, 7, 0, 4, 6, 0, 1, 4, 2, 5, 7, 1, 4, 4, 9, 9,\n",
      "        5, 6, 3, 1, 3, 5, 3, 2, 4, 3, 8, 8, 7, 0, 1, 4, 8, 5, 1, 7, 7, 9, 1, 5,\n",
      "        0, 0, 5, 5, 9, 4, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim = 1)\n",
    "print('preds:', preds)\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the predicted labels are different from the actual labels. That's because we have started with randomly initialized weights and biases. We need to train the model, i.e., adjust the weights using gradient descent to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim = 1)\n",
    "    return torch.tensor(torch.sum(preds == labels)/ preds.numel())\n",
    "\n",
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is an excellent way for us (humans) to evaluate the model. However, we can't use it as a loss function for optimizing our model using gradient descent for the following reasons:\n",
    "\n",
    "It's not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
    "\n",
    "It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these reasons, accuracy is often used as an evaluation metric for classification, but not as a loss function. A commonly used loss function for classification problems is the cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the torch.nn.functional package. Moreover, it also performs softmax internally, so we can directly pass in the model's outputs without converting them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3234, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for epoch in range(num_epochs):\\\n",
    "    for batch in train_loader:\n",
    "        # train the model\n",
    "        # compute loss\n",
    "        # cal gradients\n",
    "        # update weights\n",
    "        # reset gradients\n",
    "     for batch in val_loader:\n",
    "        # generate predictions\n",
    "        # calculate loss\n",
    "        # calculate metrics\n",
    "     #calculate average validation loss & accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " redefining the MnistModel class to include additional methods training_step, validation_step, validation_epoch_end, and epoch_end used by fit and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self.forward(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self.forward(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epochs, lr, model, train_oader, val_loader, opt_func = torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # instead of pred = model.forward() and loss = loss_fn(pred, y)\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch) \n",
    "            loss.backward() #calculates gradient of parameters\n",
    "            optimizer.step() #parameter update using current .grad values\n",
    "            optimizer.zero_grad() #as gradient accumulates in .grad so we need to make it zero after update\n",
    "    \n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the model, let's see how the model performs on the validation set with the initial set of randomly initialized weights & biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.340409755706787, 'val_acc': 0.08484968543052673}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial accuracy is around 10%, which one might expect from a randomly initialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9611, val_acc: 0.6074\n",
      "Epoch [1], val_loss: 1.6878, val_acc: 0.7291\n",
      "Epoch [2], val_loss: 1.4838, val_acc: 0.7704\n",
      "Epoch [3], val_loss: 1.3299, val_acc: 0.7870\n",
      "Epoch [4], val_loss: 1.2118, val_acc: 0.8009\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.1192, val_acc: 0.8080\n",
      "Epoch [1], val_loss: 1.0451, val_acc: 0.8150\n",
      "Epoch [2], val_loss: 0.9844, val_acc: 0.8214\n",
      "Epoch [3], val_loss: 0.9341, val_acc: 0.8263\n",
      "Epoch [4], val_loss: 0.8915, val_acc: 0.8293\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.8552, val_acc: 0.8331\n",
      "Epoch [1], val_loss: 0.8237, val_acc: 0.8353\n",
      "Epoch [2], val_loss: 0.7962, val_acc: 0.8369\n",
      "Epoch [3], val_loss: 0.7718, val_acc: 0.8394\n",
      "Epoch [4], val_loss: 0.7502, val_acc: 0.8412\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7308, val_acc: 0.8423\n",
      "Epoch [1], val_loss: 0.7133, val_acc: 0.8444\n",
      "Epoch [2], val_loss: 0.6975, val_acc: 0.8461\n",
      "Epoch [3], val_loss: 0.6830, val_acc: 0.8471\n",
      "Epoch [4], val_loss: 0.6698, val_acc: 0.8489\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy does continue to increase as we train for more epochs, the improvements get smaller with every epoch. Let's visualize this using a line graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f4a6b843828>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a700f0390>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs No of epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear from the above picture that the model probably won't cross the accuracy threshold of 90% even after training for a very long time. One possible reason for this is that the learning rate might be too high. The model's parameters may be \"bouncing\" around the optimal set of parameters for the lowest loss. You can try reducing the learning rate and training for a few more epochs to see if it helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root = 'testdata/',train = False, download = True, transform = transforms.ToTensor())\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a6b984ac8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 ,predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a7016a470>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim = 1)\n",
    "    return preds[0].item()\n",
    "\n",
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model always require batch of images as input so we input batch of size 1. img.unsqueeze simply adds another dimension at the begining of the 1x28x28 tensor, making it a 1x1x28x28 tensor, which the model views as a batch containing a single image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 ,predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADbJJREFUeJzt3X+MFPUZx/HP4xX+AYyKKQFrKhJjrMSIXgxJsaitjVUi8g9CYqURe6g1qbEkJZRYEtMEm9bGvzAQEdpQtRGMpDZiiwpFDAF/FBRsxeYa73KCBJQjmljk6R83tFe9/c6yO7szd8/7lWxud56dmScTPszMzux+zd0FIJ4zym4AQDkIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoL7SzpWZGbcTAi3m7lbP+5ra85vZDWb2dzM7YGZLmlkWgPayRu/tN7MOSf+QdL2kHkm7JM13932JedjzAy3Wjj3/VZIOuPs/3f0zSU9Kmt3E8gC0UTPhP0/S+4Ne92TT/o+ZdZnZbjPb3cS6ABSs5R/4ufsqSaskDvuBKmlmz98r6fxBr7+WTQMwDDQT/l2SLjKzyWY2WtI8SZuKaQtAqzV82O/uJ8zsXkmbJXVIWuPubxfWGYCWavhSX0Mr45wfaLm23OQDYPgi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCotg7RjdaYMWNGzdqrr76anPfiiy9O1mfNmpWs33TTTcn6c889l6yn7NixI1nfvn17w8sGe34gLMIPBEX4gaAIPxAU4QeCIvxAUIQfCKqpUXrNrFtSv6TPJZ1w986c9zNK7xDOPPPMZH39+vXJ+nXXXVez9umnnybnHT16dLI+duzYZL2V8nr/5JNPkvW77767Zu3pp59uqKfhoN5Reou4yedadz9cwHIAtBGH/UBQzYbfJb1gZq+ZWVcRDQFoj2YP+2e4e6+ZfVXSn83sHXffNvgN2X8K/McAVExTe353783+HpL0jKSrhnjPKnfvzPswEEB7NRx+MxtjZuNOPZf0XUlvFdUYgNZq5rB/gqRnzOzUcn7v7s8X0hWAlmvqOv9pr4zr/ENauXJlsr5o0aKWrXv//v3J+ocffpisHzt2rOF1ZzuOmvJ+KyBPf39/zdrVV1+dnHfPnj1NrbtM9V7n51IfEBThB4Ii/EBQhB8IivADQRF+ICgu9bXBpZdemqy//PLLyfr48eOT9Z6enpq122+/PTnvgQMHkvWPPvooWT9+/HiynnLGGel9zwMPPJCsL1u2LFnv6OioWdu4cWNy3jvvvDNZP3r0aLJeJi71AUgi/EBQhB8IivADQRF+ICjCDwRF+IGgGKK7DcaNG5es513Hz7sX46GHHqpZy7uHoEwnT55M1pcvX56s5/3s+OLFi2vW5syZk5x3zZo1yXozQ49XBXt+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK7/O3wcyZM5P1l156KVlfu3Ztsn7HHXecbkshvPfeezVrkydPTs77+OOPJ+sLFy5sqKd24Pv8AJIIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3O/zm9kaSbMkHXL3qdm0cyQ9JekCSd2S5rp7dX/IvGQPPvhgU/Pv3LmzoE5i2bx5c83aXXfdlZx3+vTpRbdTOfXs+ddKuuEL05ZI2uLuF0nakr0GMIzkht/dt0k68oXJsyWty56vk3RLwX0BaLFGz/knuHtf9vwDSRMK6gdAmzT9G37u7ql79s2sS1JXs+sBUKxG9/wHzWyiJGV/D9V6o7uvcvdOd+9scF0AWqDR8G+StCB7vkDSs8W0A6BdcsNvZk9IelXSxWbWY2YLJa2QdL2ZvSvpO9lrAMNI7jm/u8+vUfp2wb0MWxdeeGGyPmnSpGT9448/Ttb37t172j1BevHFF2vW8q7zR8AdfkBQhB8IivADQRF+ICjCDwRF+IGgGKK7ALfddluynncpcMOGDcn6jh07TrsnIA97fiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iiuv8BZg3b16ynveV3UceeaTIdoC6sOcHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaC4zt8G77zzTrK+ffv2NnUC/A97fiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IKvc6v5mtkTRL0iF3n5pNWy7ph5I+zN621N3/1Komq2DMmDE1a6NGjWpjJ0Ax6tnzr5V0wxDTf+Pul2ePER18YCTKDb+7b5N0pA29AGijZs757zWzPWa2xszOLqwjAG3RaPhXSpoi6XJJfZJ+XeuNZtZlZrvNbHeD6wLQAg2F390Puvvn7n5S0mpJVyXeu8rdO929s9EmARSvofCb2cRBL+dIequYdgC0Sz2X+p6QdI2kc82sR9LPJV1jZpdLckndkha1sEcALZAbfnefP8Tkx1rQS6XNnTu3Zm3KlCnJeQ8fPlx0O6jDzTff3PC8J06cKLCTauIOPyAowg8ERfiBoAg/EBThB4Ii/EBQ/HQ3hq0rr7wyWZ81a1bDy166dGnD8w4X7PmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICiu86Oy8q7j33///cn6WWedVbP2yiuvJOfdvHlzsj4SsOcHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaC4zl+n7u7umrX+/v72NTKCdHR0JOuLFy9O1m+99dZkvbe3t+Fl89PdAEYswg8ERfiBoAg/EBThB4Ii/EBQhB8Iytw9/Qaz8yX9VtIESS5plbs/YmbnSHpK0gWSuiXNdfejOctKr2yY2rdvX7Ket41nzpyZrFd5iO/LLrssWb/nnntq1q644orkvJ2dnQ31dMq1115bs7Z169amll1l7m71vK+ePf8JST9x929Imi7pR2b2DUlLJG1x94skbcleAxgmcsPv7n3u/nr2vF/SfknnSZotaV32tnWSbmlVkwCKd1rn/GZ2gaRpknZKmuDufVnpAw2cFgAYJuq+t9/MxkraIOk+dz9m9r/TCnf3WufzZtYlqavZRgEUq649v5mN0kDw17v7xmzyQTObmNUnSjo01LzuvsrdO929uU9vABQqN/w2sIt/TNJ+d394UGmTpAXZ8wWSni2+PQCtUs9h/zclfV/SXjN7M5u2VNIKSX8ws4WS/iVpbmtaHP4uueSSZP35559P1vv6+pL1Mk2fPj1ZHz9+fMPLzrvEuWnTpmR9165dDa87gtzwu/t2SbWuG3672HYAtAt3+AFBEX4gKMIPBEX4gaAIPxAU4QeCyv1Kb6ErG6Ff6Z0zZ06yvmzZsmR92rRpRbZTKSdPnqxZO3LkSHLehx9+OFlfsWJFQz2NdEV+pRfACET4gaAIPxAU4QeCIvxAUIQfCIrwA0Fxnb8NJk2alKznfZ9/6tSpRbZTqNWrVyfrb7zxRs3ao48+WnQ7ENf5AeQg/EBQhB8IivADQRF+ICjCDwRF+IGguM4PjDBc5weQRPiBoAg/EBThB4Ii/EBQhB8IivADQeWG38zON7OXzGyfmb1tZj/Opi83s14zezN73Nj6dgEUJfcmHzObKGmiu79uZuMkvSbpFklzJR1391/VvTJu8gFart6bfL5Sx4L6JPVlz/vNbL+k85prD0DZTuuc38wukDRN0s5s0r1mtsfM1pjZ2TXm6TKz3Wa2u6lOARSq7nv7zWyspK2SfuHuG81sgqTDklzSgxo4NbgjZxkc9gMtVu9hf13hN7NRkv4oabO7f2n0xOyI4I/unvylScIPtF5hX+wxM5P0mKT9g4OffRB4yhxJb51ukwDKU8+n/TMk/VXSXkmnxlteKmm+pMs1cNjfLWlR9uFgalns+YEWK/SwvyiEH2g9vs8PIInwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVO4PeBbssKR/DXp9bjatiqraW1X7kuitUUX29vV639jW7/N/aeVmu929s7QGEqraW1X7kuitUWX1xmE/EBThB4IqO/yrSl5/SlV7q2pfEr01qpTeSj3nB1Cesvf8AEpSSvjN7AYz+7uZHTCzJWX0UIuZdZvZ3mzk4VKHGMuGQTtkZm8NmnaOmf3ZzN7N/g45TFpJvVVi5ObEyNKlbruqjXjd9sN+M+uQ9A9J10vqkbRL0nx339fWRmows25Jne5e+jVhM/uWpOOSfntqNCQz+6WkI+6+IvuP82x3/2lFeluu0xy5uUW91RpZ+gcqcdsVOeJ1EcrY818l6YC7/9PdP5P0pKTZJfRRee6+TdKRL0yeLWld9nydBv7xtF2N3irB3fvc/fXseb+kUyNLl7rtEn2Voozwnyfp/UGve1StIb9d0gtm9pqZdZXdzBAmDBoZ6QNJE8psZgi5Ize30xdGlq7MtmtkxOui8YHfl81w9yskfU/Sj7LD20rygXO2Kl2uWSlpigaGceuT9Osym8lGlt4g6T53Pza4Vua2G6KvUrZbGeHvlXT+oNdfy6ZVgrv3Zn8PSXpGA6cpVXLw1CCp2d9DJffzX+5+0N0/d/eTklarxG2XjSy9QdJ6d9+YTS592w3VV1nbrYzw75J0kZlNNrPRkuZJ2lRCH19iZmOyD2JkZmMkfVfVG314k6QF2fMFkp4tsZf/U5WRm2uNLK2St13lRrx297Y/JN2ogU/835P0szJ6qNHXhZL+lj3eLrs3SU9o4DDw3xr4bGShpPGStkh6V9JfJJ1Tod5+p4HRnPdoIGgTS+pthgYO6fdIejN73Fj2tkv0Vcp24w4/ICg+8AOCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ENR/AAuNb1TcRWGLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a74946b38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[10]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9 ,predicted: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a749b3198>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[193]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2 ,predicted: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a74868c50>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[1839]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.\n",
    "\n",
    "As a final step, let's also look at the overall loss and accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6443870663642883, 'val_acc': 0.864815890789032}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle = True, batch_size = 1024)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0228,  0.0044,  0.0320,  ...,  0.0190, -0.0125,  0.0334],\n",
       "                      [ 0.0162, -0.0049, -0.0139,  ...,  0.0027,  0.0168, -0.0010],\n",
       "                      [-0.0354,  0.0040, -0.0228,  ..., -0.0122, -0.0245, -0.0249],\n",
       "                      ...,\n",
       "                      [ 0.0214,  0.0108,  0.0345,  ..., -0.0318, -0.0122, -0.0131],\n",
       "                      [-0.0127,  0.0172,  0.0006,  ...,  0.0095, -0.0199, -0.0151],\n",
       "                      [ 0.0121, -0.0302,  0.0143,  ...,  0.0132, -0.0077, -0.0318]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0570,  0.0569,  0.0022, -0.0125,  0.0010,  0.0357, -0.0333,  0.0636,\n",
       "                      -0.1027, -0.0329]))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .state_dict method returns an OrderedDict containing all the weights and bias matrices mapped to the right attributes of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0118,  0.0084, -0.0004,  ..., -0.0097, -0.0250,  0.0311],\n",
       "                      [ 0.0080,  0.0084,  0.0069,  ..., -0.0092, -0.0096,  0.0097],\n",
       "                      [-0.0278,  0.0345,  0.0055,  ..., -0.0186,  0.0141,  0.0165],\n",
       "                      ...,\n",
       "                      [-0.0252,  0.0085, -0.0052,  ..., -0.0268, -0.0118, -0.0190],\n",
       "                      [ 0.0262,  0.0207,  0.0227,  ..., -0.0231,  0.0292, -0.0260],\n",
       "                      [-0.0146, -0.0337,  0.0020,  ..., -0.0155, -0.0282, -0.0068]])),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0261,  0.0018,  0.0213, -0.0091, -0.0070,  0.0284,  0.0129,  0.0120,\n",
       "                       0.0313, -0.0132]))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MnistModel()\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3220322132110596, 'val_acc': 0.11022601276636124}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0228,  0.0044,  0.0320,  ...,  0.0190, -0.0125,  0.0334],\n",
       "                      [ 0.0162, -0.0049, -0.0139,  ...,  0.0027,  0.0168, -0.0010],\n",
       "                      [-0.0354,  0.0040, -0.0228,  ..., -0.0122, -0.0245, -0.0249],\n",
       "                      ...,\n",
       "                      [ 0.0214,  0.0108,  0.0345,  ..., -0.0318, -0.0122, -0.0131],\n",
       "                      [-0.0127,  0.0172,  0.0006,  ...,  0.0095, -0.0199, -0.0151],\n",
       "                      [ 0.0121, -0.0302,  0.0143,  ...,  0.0132, -0.0077, -0.0318]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0570,  0.0569,  0.0022, -0.0125,  0.0010,  0.0357, -0.0333,  0.0636,\n",
       "                      -0.1027, -0.0329]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.643673300743103, 'val_acc': 0.8652044534683228}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model2, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
