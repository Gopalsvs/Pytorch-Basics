{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(root = 'traindata/', download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 60,000 images that we'll use to train the model. There is also an additional test set of 10,000 images used for evaluating models and reporting metrics in papers and reports. We can create the test dataset using the MNIST class by passing train=False to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root = 'testdata/',train = False, download = True)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7F3AF31B3470>, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pair, consisting of a 28x28 px image and a label. The image is an object of the class PIL.Image.Image, which is a part of the Python imaging library Pillow. We can view the image within Jupyter using matplotlib, the de-facto plotting and graphing library for data science in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3af31b3c50>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image, label = dataset[1000]\n",
    "plt.imshow(image, cmap = 'gray')\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch datasets allow us to specify one or more transformation functions that are applied to the images as they are loaded. The torchvision.transforms module contains many such predefined functions. We'll use the ToTensor transform to convert images into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms \n",
    "dataset = MNIST(root = 'traindata/', train = True, transform = transforms.ToTensor())\n",
    "\n",
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is now converted to a 1x28x28 tensor. The first dimension tracks color channels. The second and third dimensions represent pixels along the height and width of the image, respectively. Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in which case there are three channels: red, green, and blue (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[0, 10:15, 10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3aebc49b38>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3aebb87278>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0, 10:15, 10:15], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets\n",
    "\n",
    "Training set - used to train the model, i.e., compute the loss and adjust the model's weights using gradient descent.\n",
    "\n",
    "Validation set - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model.\n",
    "\n",
    "Test set - used to compare different models or approaches and report the model's final accuracy.\n",
    "In the MNIST dataset, there are 60,000 training images and 10,000 test images. The test set is standardized so that different researchers can report their models' results against the same collection of images.\n",
    "\n",
    "Since there's no predefined validation set, we must manually split the 60,000 images into training and validation datasets. Let's set aside 10,000 randomly chosen images for validation. We can do this using the random_spilt method from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's essential to choose a random sample for creating a validation set. Training data is often sorted by the target labels, i.e., images of 0s, followed by 1s, followed by 2s, etc. If we create a validation set using the last 20% of images, it would only consist of 8s and 9s. In contrast, the training set would contain no 8s or 9s. Such a training-validation would make it impossible to train a useful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader #with dataloader check tensordataset which is used to combine data and labels.\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, shuffle = True, batch_size = 128)\n",
    "val_loader = DataLoader(val_ds, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0265,  0.0321,  0.0208,  ...,  0.0239, -0.0315, -0.0265],\n",
      "        [ 0.0062,  0.0154,  0.0212,  ..., -0.0131,  0.0181, -0.0150],\n",
      "        [-0.0140,  0.0096, -0.0348,  ...,  0.0318, -0.0130, -0.0002],\n",
      "        ...,\n",
      "        [-0.0044, -0.0301, -0.0257,  ..., -0.0024, -0.0351,  0.0026],\n",
      "        [ 0.0314, -0.0104, -0.0119,  ..., -0.0027, -0.0078,  0.0276],\n",
      "        [ 0.0001, -0.0336, -0.0281,  ..., -0.0056,  0.0101, -0.0276]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0162, -0.0284, -0.0022,  0.0055,  0.0157, -0.0118,  0.0079, -0.0218,\n",
      "        -0.0181, -0.0178], requires_grad=True)]\n",
      "weight shape: torch.Size([10, 784])\n",
      "bias shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "print(list(model.parameters()))\n",
    "print(\"weight shape:\", model.weight.shape)\n",
    "print(\"bias shape:\", model.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model weights shape: torch.Size([10, 784])\n",
      "model bias shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"model weights shape:\",model.linear.weight.shape)\n",
    "print(\"model bias shape:\",model.linear.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([128, 10])\n",
      "Sample output: tensor([[ 0.1932,  0.2198,  0.1162, -0.0137, -0.0994,  0.1978,  0.0163,  0.0714,\n",
      "          0.0233, -0.0859]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model.forward(images)\n",
    "    break\n",
    "\n",
    "print('Output shape:', outputs.shape)\n",
    "print('Sample output:', outputs[:1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities: tensor([[0.1131, 0.1162, 0.1047, 0.0920, 0.0844, 0.1136, 0.0948, 0.1001, 0.0954,\n",
      "         0.0856]], grad_fn=<SliceBackward>)\n",
      "Sum: 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(outputs, dim = 1)\n",
    "print(\"Sample probabilities:\", probs[:1])\n",
    "\n",
    "print(\"Sum:\", torch.sum(probs[1]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: tensor([1, 1, 1, 1, 2, 3, 7, 5, 3, 5, 1, 3, 3, 5, 3, 1, 2, 5, 3, 3, 5, 1, 1, 5,\n",
      "        1, 1, 3, 3, 5, 3, 3, 1, 1, 2, 5, 1, 1, 6, 1, 3, 1, 7, 3, 5, 4, 2, 3, 1,\n",
      "        3, 1, 1, 5, 1, 3, 5, 5, 5, 3, 2, 4, 4, 2, 4, 1, 5, 3, 4, 5, 5, 2, 3, 3,\n",
      "        7, 4, 5, 3, 5, 1, 1, 6, 5, 7, 3, 5, 3, 3, 3, 2, 2, 5, 3, 5, 5, 1, 3, 5,\n",
      "        2, 5, 1, 5, 5, 7, 2, 3, 4, 3, 1, 2, 3, 3, 4, 3, 7, 3, 4, 1, 1, 1, 2, 2,\n",
      "        5, 3, 4, 1, 3, 5, 3, 5])\n",
      "labels: tensor([6, 4, 4, 7, 4, 4, 4, 9, 8, 6, 5, 0, 0, 3, 5, 1, 8, 7, 8, 0, 4, 1, 1, 7,\n",
      "        7, 1, 8, 3, 9, 0, 7, 7, 3, 8, 5, 1, 5, 0, 1, 3, 1, 7, 7, 9, 6, 9, 9, 5,\n",
      "        3, 1, 1, 9, 1, 5, 4, 0, 6, 3, 9, 0, 8, 4, 2, 5, 6, 3, 0, 4, 6, 4, 3, 0,\n",
      "        9, 0, 4, 9, 9, 6, 1, 6, 1, 7, 9, 9, 0, 4, 5, 3, 3, 7, 2, 4, 5, 6, 0, 4,\n",
      "        8, 1, 6, 6, 4, 7, 8, 0, 2, 3, 1, 4, 3, 3, 5, 1, 7, 4, 0, 6, 1, 6, 4, 8,\n",
      "        6, 7, 2, 5, 9, 9, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim = 1)\n",
    "print('preds:', preds)\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the predicted labels are different from the actual labels. That's because we have started with randomly initialized weights and biases. We need to train the model, i.e., adjust the weights using gradient descent to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2266)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim = 1)\n",
    "    return torch.tensor(torch.sum(preds == labels)/ preds.numel())\n",
    "\n",
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is an excellent way for us (humans) to evaluate the model. However, we can't use it as a loss function for optimizing our model using gradient descent for the following reasons:\n",
    "\n",
    "It's not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
    "\n",
    "It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these reasons, accuracy is often used as an evaluation metric for classification, but not as a loss function. A commonly used loss function for classification problems is the cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the torch.nn.functional package. Moreover, it also performs softmax internally, so we can directly pass in the model's outputs without converting them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2894, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for epoch in range(num_epochs):\\\n",
    "    for batch in train_loader:\n",
    "        # train the model\n",
    "        # compute loss\n",
    "        # cal gradients\n",
    "        # update weights\n",
    "        # reset gradients\n",
    "     for batch in val_loader:\n",
    "        # generate predictions\n",
    "        # calculate loss\n",
    "        # calculate metrics\n",
    "     #calculate average validation loss & accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " redefining the MnistModel class to include additional methods training_step, validation_step, validation_epoch_end, and epoch_end used by fit and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self.forward(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self.forward(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # instead of pred = model.forward() and loss = loss_fn(pred, y)\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch) \n",
    "            loss.backward() #calculates gradient of parameters\n",
    "            optimizer.step() #parameter update using current .grad values\n",
    "            optimizer.zero_grad() #as gradient accumulates in .grad so we need to make it zero after update\n",
    "    \n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the model, let's see how the model performs on the validation set with the initial set of randomly initialized weights & biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3055222034454346, 'val_acc': 0.10462816804647446}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial accuracy is around 10%, which one might expect from a randomly initialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9355, val_acc: 0.6300\n",
      "Epoch [1], val_loss: 1.6673, val_acc: 0.7393\n",
      "Epoch [2], val_loss: 1.4675, val_acc: 0.7727\n",
      "Epoch [3], val_loss: 1.3170, val_acc: 0.7885\n",
      "Epoch [4], val_loss: 1.2012, val_acc: 0.7998\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.1102, val_acc: 0.8087\n",
      "Epoch [1], val_loss: 1.0373, val_acc: 0.8177\n",
      "Epoch [2], val_loss: 0.9777, val_acc: 0.8241\n",
      "Epoch [3], val_loss: 0.9281, val_acc: 0.8294\n",
      "Epoch [4], val_loss: 0.8861, val_acc: 0.8331\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.8501, val_acc: 0.8359\n",
      "Epoch [1], val_loss: 0.8190, val_acc: 0.8398\n",
      "Epoch [2], val_loss: 0.7916, val_acc: 0.8428\n",
      "Epoch [3], val_loss: 0.7675, val_acc: 0.8452\n",
      "Epoch [4], val_loss: 0.7461, val_acc: 0.8471\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7268, val_acc: 0.8496\n",
      "Epoch [1], val_loss: 0.7095, val_acc: 0.8511\n",
      "Epoch [2], val_loss: 0.6936, val_acc: 0.8535\n",
      "Epoch [3], val_loss: 0.6792, val_acc: 0.8553\n",
      "Epoch [4], val_loss: 0.6661, val_acc: 0.8565\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_loader, val_loader, opt_func = torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the accuracy does continue to increase as we train for more epochs, the improvements get smaller with every epoch. Let's visualize this using a line graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f3ae731a278>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8XHW9//HXJ3ubJmlL0r2lK4WyFipgVVAERLgXxIULRQUXuCh1AbmK4kUeuF03vHrhiuhFUCiLG/aH1bIoKFaggdZKW0uTWtq0SZt0mTRtlib5/P44J8M0nUmmaU8mybyfj8c8Mmf/5Ezy/cz5fs/3fM3dERERAcjJdAAiIjJwKCmIiEickoKIiMQpKYiISJySgoiIxCkpiIhInJKCyCBlZl8xswYzq8t0LABmdpuZPZDpOOTwKClIr8zsGTPbZWaFmY5lIDOz+8zMzez0hHkzzeyIdwYysynAZ4A57j7uSO9fspeSgvTIzKYCbwEcuLifj53Xn8c7QnYCX+mH40wBdrj79n44lmQRJQXpzQeB54H7gKsSF5jZMDP7jpm9ZmYxM3vOzIaFy95sZsvMbLeZbTazq8P5z5jZRxP2cbWZPZcw7WZ2vZmtB9aH874X7qPRzF4ys7ckrJ9rZl8ws2oz2xMun2xmd5nZd7rFu9jMbuj+C5rZD8zs293m/cbMbgzff87MtoT7X2dmb+/hfN0PnGRmZydbaGYTwjh2mlmVmV2TakdmVmZmPzWz+vAcf9HMcszsXOBJYIKZNZnZfSm2/xczWxl+BsvM7KSEZRvN7PNmtia8CvyJmRUlLL8mjG9nGO+EhGXHm9mT4bJtZvaFhMMWhDHvMbPVZjYvYbtDOY+SKe6ul14pX0AV8HHgNGA/MDZh2V3AM8BEIBeYDxQCRwN7gCuAfOAo4JRwm2eAjybs42rguYRpJyjwRgPDwnnvD/eRR1BlUgcUhcv+A/g7MBsw4ORw3dOBrUBOuF45sC8x/oRjngVsBiycHgU0AxPC/W4GJoTLpgIzUpyr+wiuEj7Z9TsBM4N/s/g6fwL+FygCTgHqgXNS7O+nwG+AkvC4rwIfCZe9Fajp4XObC2wHzgg/m6uAjUBhuHwj8AowOTzXfwG+Ei47B2gATg0/z/8B/hQuKwFqw8+hKJw+I1x2G9ACXBge8+vA8+GytM+jXhn+n890AHoN3BfwZoJEUB5O/wO4IXyfExacJyfZ7vPAr1Ps8xl6TwpJC8mEdXZ1HRdYB1ySYr21wHnh+4XAkhTrGbAJOCucvgb4Q/h+Zli4ngvk9xJXV1IoDPf3zsSkEBbAHUBJwjZfB+5Lsq9coI2gzaBr3r8Dz4Tve0sKPwC+3G3eOuDs8P1G4LqEZRcC1eH7/wO+mbBsRPh3MJUg0a9IcczbgKcSpucAzYd6HvXK7EvVR9KTq4An3L0hnF7E61VI5QTfFKuTbDc5xfx0bU6cMLObzGxtWEW1GygLj9/bse4nuMog/PmzZCt5UGo9TFDgASwAHgyXVQGfJijwtpvZw4lVKSn21wp8OXwlmgDsdPc9CfNeI7jS6q6c4CrrtTTWTeZo4DNh1dHu8LxNDmPoknieX0tYNiHxuO7eBOwIj93bZ5t4J9Q+oMjM8vpyHiUzlBQkqbBt4DLgbDOrC297vAE42cxOJqheaAFmJNl8c4r5AHuB4QnTye6cid+tE7YffDaMZZS7jwRiBN/uezvWA8AlYbzHAY+lWA/gIeC9ZnY0QZXLL+PBuC9y9zcTFLQOfKOH/XT5CTASeHfCvK3AaDMrSZg3BdiSZPsGgm/nR6exbjKbga+6+8iE13B3fyhhncnd9r01Ic74cc2smKBKbku43+lpxnCAPp5H6WdKCpLKuwiqOuYQ1H2fQlCw/hn4oLt3AvcCd4SNp7lm9sbwttUHgXPN7DIzyzOzo8zslHC/K4F3m9lwM5sJfKSXOEqAdoK69zwzuxUoTVj+Y+DLZjbLAieZ2VEA7l4DLCe4QviluzenOoi7ryAoiH8MLHX33QBmNtvMzgl/rxaCKrPO3k6eu7cDXwI+lzBvM7AM+LqZFYUNvx8hSF7dt+8AHgW+amYlYbK6Mdm6KfwIuM7MzgjPS7GZXdQtIV1vZpPMbDRwC/BIOP8h4ENmdkr4e38NeMHdNwKPA+PN7NNmVhjGdkZvwfT1PEr/U1KQVK4CfuLum9y9rusF3AlcacHtojcRNPIuJ7gV8xsEDbubCOqoPxPOX0nQAAzwXYK68m0E1TsP9hLHUuD3BI2srxEUKInVHncQFJ5PAI0E9eHDEpbfD5xIiqqjbhYR1HkvSphXCPwXQcKoA8YQtJmk4yGCRtlEVxDUzW8Ffg18yd2fSrH9JwiurDYAz4Vx3ZvOgd29kqBt5E6CNpgqgvabRIsIztsGgiqhr4TbPgX8J8HVUi3Bldjl4bI9wHnAvxKcj/XA29II6XDOo/SjrrstRIYkMzuL4Nv10a4/9jgz20jQ4J8qIUmW0pWCDFlmlg98CvixEoJIepQUZEgys+OA3cB44L8zHI7IoKHqIxERidOVgoiIxA26B46Vl5f71KlTMx2GiMig8tJLLzW4e0Vv6w26pDB16lQqKyszHYaIyKBiZq/1vpaqj0REJIGSgoiIxCkpiIhInJKCiIjEKSmIiEickoKIyAB297PVLKtuOGDesuoG7n72cIYsSU1JQUSkF4dTMB9uoX7SpDIWLloR38ey6gYWLlrBSZPK0oz+0Ay6fgoikr3ufraakyaVMX9GeXzesuoGVtXEuO7sVGMtHf62XQXznQvmMn9GebxgvnPB3KTrt3d00toevCaNGsbHHniZWy46juPGlfLSpl1854l1fOKcmTyzbjv7O5z2jk72d4Y/Ozrj89o7nbaOTs45dgwfvm85F588gafWbo/HEYVB9+yjefPmuTqviWTO4RSuh7t9YmHcvXDurZD88/p6PvnQCr75npM5aXIZf6lq4LbFq7nxvGOYXjGCfW0dNO9vD362dYTTXe/b2bRzHy+9touxpUXUxVqYNHoY+Tk5tLZ30tbeSWt7RzwRdHRGV65+8pyZ3Hj+7EPezsxecvd5va6npCCSfTJVMMe3f3AFd1x2MidOKuO59Q3cung1N51/DDPHlMQL5gMK57Zw3v4ONu3Yxwv/3MGEkcPYsquZGRXFFBXk0dYefMtO/NmWMN2Xcjo/1xiWn8vwgjyGF+Syp2U/9U1tTBo1jGPGllCYlxO+cinMf/19QXx+DoX5uRTm5fDkmm387pU6Lj55Av/2hsnk5Rh5uTkU5OaQl2vk5xp5OTnk5+WQHy7LyzUKcnNYvnEnn3p4Je8/YwoPvLCpT1cKSgoiQ1wUBfsd7wsK6q5vyfvCb8nNBxTS7ayt28NvVm7huHGlrKlt5MzpoykbVkBrewct+xO+Ne/vpKW9g9b9XdUpwfu2jkMfiXN4QS7DC3IZVpDL3tZ2du7dz/iyIqYeVUx+XlC4FublkJ9rFOTlkJ+bQ0E4v+tnfl4Of6lq4M/rGzh/zljefeqkA/bbVfgX5Qfz8nNfb3btOkd9KZiPxLZ9TcJdlBRE+kGmq1Kuf/BlvnbpiRw7vpS/Vjfw9d/9g38/azqTRw+nqbWdva3tNLV2sDd8vyf8ube1nW2NrdTs2kdhXg7N+/s+XHJRfg6jhhdQmJdDUfit+PVvzgd+gy7qmpeXw4sbd/LX6h28/dgxvGvuxIMK5mFhwTy8II+i/BzMLP5793fhfDgF8+EW6of7N9ZFSUEkTRmtSqlq4PpFL/PVS0/gmLGlPFdVz7eXvsoH33g0Y0oKiTW3E2veH381JryPNe+neX9HWr9jcUEuI4ryKC7MY0RhHsUFwfvNO/eyblsTcyeP5C3HVMQL4mEFua9/g87PO+jb9Kqa3dz46N/6XJ0x2ArnTDVwH0lKCiJp6kth0bK/g8bm/TS27Oe5qh18e+k63jTzKP68voFL506kfEQh+9pe/5YevG9nb9e39rbwfVs7vf0LFhfkUjYsn9Jh+ZQleVW+totnX63nohPHc+WZU4JCv6vwL8xjeH4uOTmW8vfuz4L5cLcfCoVzpigpSFbp6z/8vrZ2Gva08Yd12/j20ld5w9TRPL9hB+fNGUtJUR6NLe3xb+iNLftpbG6nsWU/be29V7cMy8+luDCP4sJcigu6CulchhfmMaKgq+AOpp/fsINn1tVz8ckT+NCbpsYL/NJh+QfUa3eXiYI9k1Vm0ndKCjLoHKlqnFOnjOKJ1XV88bFXuOas6RxVXEhDUyv1e1oP+rm3LXn1S16OxQvl0qK84OewfEqL8ikdlhf+DAruLbv28YNnqrn45Ak8vqqWb192Mm+bPYbcJN/Oe4p9MBXsMvgoKcigk04h19TaTl2smdpYC7WxFuriP5up2t5Eza5mUv1Flw3Lp6KkkPIRBVSUFFExopDykgIqRhRS39TK3c9U857TJvHYii3cteBU5s8culUpkn2UFGTQ6ex0Hl+1lVsee4U3TB3NsuodnDltFB0OdWEC2NPaftB2RxUXMK6siPFlRWxrbOHvWxo559gxXHnGFMpHFFJRUshRIwoozMtNelxVpUg2UFKQjEinkGtu62BDQxPV9Xup3t5EdX0TG+r3sqGhiZZut0aOLS1kXNkwxpcWxQv+4OcwxpcVMaa0MF7Y97UaRgWzZAMlBcmI+LfsK+YyY8wIFq/cwnefWs9bZpWzr62DDfV72bK7Ob6+GUweNZzpFcXMqBgBwKOVm3nvqZN4bOUW7rry1H65I0ZkqBsQScHMLgC+B+QCP3b3/+q2fApwPzAyXOdmd1/S0z6VFKJ1qN+aW/Z3sHHHXqq372VDffCtf1VNjA0New9Yb3hBLjMqRjCjopjpFSOC92OKmXpUMUX5B37TV/26yJGX8aRgZrnAq8B5QA2wHLjC3dckrHMPsMLdf2Bmc4Al7j61p/0qKUQrVcH85UuOZ1RxARvq98are6rrm9iyu/mA++wnlBUxvWIEjc37WbUlxrtOmcjn3jmbcaVF8R6pqahgF4lOukkhykdnnw5UufuGMKCHgUuANQnrOFAavi8DtkYYj6Th+AllfOzs6Xzk/kqmjB5O1fYm8nON6xetiK8zLD+X6RXFzJ0yiveeNonpFSOYXl7M9IpihhfkxRPJJ8+ZyQMvbOKyN0xifNmwXo+drOCfP6Nc1T8i/SjKpDAR2JwwXQOc0W2d24AnzOwTQDFwbrIdmdm1wLUAU6ZMOeKBDjXpfuPeubeNV7bEeGVrLPi5pZFNO/fFl6+r28Pk0cM4Z/YYZowZwfTyEUyvKGZcaVHSHrJdx0m80jhzxlGq2xcZRDI9yM4VwH3u/h0zeyPwMzM7wd0PuAXF3e8B7oGg+igDcQ4qyQYE+fgDL3PNWdP4/tPrwwQQY2usJb7NlNHDOWFiKZefPpkcM+5+tpoPnnk0D7ywiXecMC7tAn1VTeyABDB/Rjl3LpjLqpqYkoLIIBBlm8Ibgdvc/R3h9OcB3P3rCeusBi5w983h9AbgTHffnmq/alPoXWen89CLm/jKb9cytrSQ13buO6Def3p5MSdMLOOEiaWcMKGM4yeUUTY8H9BdPCJD1UBoU1gOzDKzacAW4HJgQbd1NgFvB+4zs+OAIqA+wpiGrK27m3lufQN/Wl/Psuod7NzbBsDGHfs4dlwJ75s3mRMnlnHc+BJKivJT7kff9EWyW2RJwd3bzWwhsJTgdtN73X21md0OVLr7YuAzwI/M7AaCRuerfbB1nIhIb+0Ce1vbeeGfO/jTqw38eX091fXBLaAVJYW89ZgKxpUVsejFTfEqoOPGl3D6tNG9HleNvSLZTZ3XBqju1TbPrW/g4w++xAUnjOO1Hft4edMu9nc4hXk5nDH9KM6aVc6bZ5Uze2wJf92wQ1VAInKAjPdTiEq2JAUIBmC59mcvMa6siOrtTfEHvR0/oZQ3zyrnrFkVnHb0qHjnry66319EuhsIbQpyGP5eE+OOJ1+lqbWdqu1NHDe+hOvOnsGbZpZTPqKwx21VBSQifaWkMMBs39PCt5eu4+cv1VBSmEdxQS4fetM0Fr24KXzsc88JQUTkcCgpDBCt7R3c95eN/M8fqmht7+CiE8fzl6oG7v7AacG3/JnqBCYi0VNSyDB35+m12/nKb9ewccc+3n7sGG656DieWLONBWdM0a2hItKvlBQy6NVte/jy42v48/oGZo4Zwf0fPp2zj6kA4LqzRxy0vtoFRCRqSgoZsHtfG//91Hp+9vxrFBfk8qV/ncP7zzy6xwHaRUT6g5JChLrfGtre0clXfruWh5dvoq29kwVnTOHG82Yzurggw5GKiASUFCKU+GA6d/jcL/9Gza4W5owv5TuXncxx40t734mISD9SUojQ/BnlfP/yuVx973LaOjrJMbjh3GP45Ntn9jrgjIhIJigpRKy+qYW2juBJ4NedPYNPnTsrwxGJiKSmls0ItbV38rUla8nNMT7xtpk8vHwzy6obMh2WiEhKSgoR+tqStdTvaeOm84/hM++YzZ0L5rJw0QolBhEZsJQUIrKvrZ1HKzdz7LiS+LOIEjugiYgMRGpTiMhP/rKRfW0dfPXSEw5oVFYHNBEZyHSlEIFde9u4+5lqzj1uLKcd3fvANiIiA4WSQgTufraaprZ2/uMdszMdiojIIYk0KZjZBWa2zsyqzOzmJMu/a2Yrw9erZrY7ynj6Q12shfuWbeTSUyYye1xJpsMRETkkkbUpmFkucBdwHlADLDezxe6+pmsdd78hYf1PAHOjiqe/fO/p9XS6c8N5x2Q6FBGRQxbllcLpQJW7b3D3NuBh4JIe1r8CeCjCeCL3z4a9PFq5mQWnT2Hy6OGZDkdE5JBFmRQmApsTpmvCeQcxs6OBacAfUiy/1swqzayyvr7+iAd6pHzniXUU5uWw8Bz1WhaRwWmgNDRfDvzC3TuSLXT3e9x9nrvPq6io6OfQ0vPKlhiPr6rlw2+aRkWJhswUkcEpyqSwBZicMD0pnJfM5QzyqqNvLV3HyOH5XHv29EyHIiLSZ1EmheXALDObZmYFBAX/4u4rmdmxwCjgrxHGEqnnN+zg2Vfr+djZMygtys90OCIifRZZUnD3dmAhsBRYCzzq7qvN7HYzuzhh1cuBh93do4olSu7ON3//D8aWFnLV/KmZDkdE5LBE+pgLd18CLOk279Zu07dFGUPUnlq7nZc37ebr7z6RovzcTIcjInJYBkpD86DU0el8a+k/mFZezPtOm5TpcEREDpuSwmH4zcotvLqtic+cfwx5uTqVIjL4qSTro9b2Du548lWOn1DKhSeMz3Q4IiJHhJJCHz30wiZqdjXz2QuOJSdH4y2LyNCgpNAHe1vbufOPVZw5fTRnzdLYCCIydCgp9MG9z/2ThqY2PnvBsQcMoCMiMtgpKRyiXXvbuOdPGzhvzlhOnTIq0+GIiBxRSgqH6AfhADo3na8BdERk6FFSOAS1seZgAJ25GkBHRIYmJYVD8P2n1+Pu3HCuBtARkaFJSaEXdz9bzbLqBqrrm3i0soYrzziazbv2cfez1ZkOTUTkiIv02UdDwUmTyli4aAWzxoygMC+HM6aNZuGiFdy5YNCPHCoichBdKfRi/oxyvnDhsbzwz50cP6GUWx57hTsXzGX+DPVPEJGhR0khDcUFwQXV8o27eP8ZU5QQRGTIUlJIw7LqHQBc85ZpPPDCJpZVN2Q4IhGRaCgp9GJZdQM/r9xMfo7xhQuP484Fc1m4aIUSg4gMSUoKvVhVE+OUKSOZOGoYZsb8GeXcuWAuq2pimQ5NROSIU1LoxXVnz6C9wxlXVhSfN39GOdedPSODUYmIRCPSpGBmF5jZOjOrMrObU6xzmZmtMbPVZrYoynj6qjbWwviyYZkOQ0QkcpH1UzCzXOAu4DygBlhuZovdfU3COrOAzwNvcvddZjYmqnj6qrPT2dbYwviEKwURkaEqyiuF04Eqd9/g7m3Aw8Al3da5BrjL3XcBuPv2COPpk4amVto7XUlBRLJClElhIrA5YbomnJfoGOAYM/uLmT1vZhck25GZXWtmlWZWWV9fH1G4ydXGWgAYp+ojEckCmW5ozgNmAW8FrgB+ZGYju6/k7ve4+zx3n1dRUdGvAXYlBV0piEg2iDIpbAEmJ0xPCuclqgEWu/t+d/8n8CpBkhgw6mLNAAfcfSQiMlRFmRSWA7PMbJqZFQCXA4u7rfMYwVUCZlZOUJ20IcKYDlltYwsFuTkcVVyQ6VBERCIXWVJw93ZgIbAUWAs86u6rzex2M7s4XG0psMPM1gB/BP7D3XdEFVNf1O5uYVxZkcZiFpGsEOmjs919CbCk27xbE947cGP4GpDqYi2qOhKRrJHphuYBr7axWY3MIpI1lBR60NnpbIu16kpBRLKGkkIPdu5ro62jkwnqoyAiWUJJoQe1u7s6rulKQUSyg5JCD2rDPgpqUxCRbJFWUjCzX5nZRWaWVUmkrlFXCiKSXdIt5P8XWACsN7P/MrPZEcY0YNTGWsjPNcqLCzMdiohIv0grKbj7U+5+JXAqsBF4ysyWmdmHzCw/ygAzqS7WwtjSInJy1HFNRLJD2tVBZnYUcDXwUWAF8D2CJPFkJJENAFt3q4+CiGSXtHo0m9mvgdnAz4B/dffacNEjZlYZVXCZVtfYwkmTDnpoq4jIkJXuYy6+7+5/TLbA3ecdwXgGDHenNtbCO47XlYKIZI90q4/mJI5zYGajzOzjEcU0IOzat5+29k7GlSopiEj2SDcpXOPuu7smwuEzr4kmpIFh6+6gj8KEkUoKIpI90k0KuZbw7GgzywWG9AADdRqGU0SyULptCr8naFT+YTj97+G8Iau2UcNwikj2STcpfI4gEXwsnH4S+HEkEQ0QdbFmcnOM8hHquCYi2SOtpODuncAPwldWqI21MLakkFx1XBORLJLus49mmdkvzGyNmW3oeqWx3QVmts7Mqszs5iTLrzazejNbGb4+2pdfIgq1u1sYP1LtCSKSXdJtaP4JwVVCO/A24KfAAz1tEDZG3wW8E5gDXGFmc5Ks+oi7nxK+BkyVVF2jhuEUkeyTblIY5u5PA+bur7n7bcBFvWxzOlDl7hvcvQ14GLik76H2n6DjWjPj1UdBRLJMukmhNXxs9nozW2hmlwIjetlmIrA5YbomnNfde8xsVVg9NTnZjszsWjOrNLPK+vr6NEPuu1jzflr2d+pKQUSyTrpJ4VPAcOCTwGnA+4GrjsDx/x8w1d1PIrij6f5kK7n7Pe4+z93nVVRUHIHD9qw21nU7qtoURCS79Hr3Udg28G/ufhPQBHwozX1vARK/+U8K58W5+46EyR8D30xz35GKj7im3swikmV6vVJw9w7gzX3Y93JglplNM7MC4HJgceIKZjY+YfJiYG0fjnPEvX6loKQgItkl3c5rK8xsMfBzYG/XTHf/VaoN3L3dzBYCS4Fc4F53X21mtwOV7r4Y+KSZXUxwV9NOgvEaMq4u1kKOQYU6rolIlkk3KRQBO4BzEuY5kDIpALj7EmBJt3m3Jrz/PPD5NGPoN7WxFsaUFJGXm1VDUouIpN2jOd12hCGhLqY+CiKSndIdee0nBFcGB3D3Dx/xiAaArbFmjh1XkukwRET6XbrVR48nvC8CLgW2HvlwMs/dqYu18NZjxmQ6FBGRfpdu9dEvE6fN7CHguUgiyrDGlnb2tXXoziMRyUp9bUmdBQzJr9KvD66jpCAi2SfdNoU9HNimUEcwxsKQ09VxTcNwikg2Srf6KGtaXWs1DKeIZLF0x1O41MzKEqZHmtm7ogsrc2pjLZjBmBJ1XBOR7JNum8KX3D3WNeHuu4EvRRNSZtXFmqkYUUi+Oq6JSBZKt+RLtl66t7MOKrWxFt15JCJZK92kUGlmd5jZjPB1B/BSlIFlSl2sRY/MFpGslW5S+ATQBjxCMIJaC3B9VEFlUq0ecSEiWSzdu4/2AjdHHEvG7WnZT1Nru6qPRCRrpXv30ZNmNjJhepSZLY0urMxQxzURyXbpVh+Vh3ccAeDuuxiCPZo1DKeIZLt0k0KnmU3pmjCzqSR5aupgV6cR10Qky6V7W+ktwHNm9ixgwFuAayOLKkO2ho+4GFuqpCAi2SmtKwV3/z0wD1gHPAR8BmjubTszu8DM1plZlZmlbKg2s/eYmZvZvDTjjkRdrIXyEYUU5Knjmohkp3QfiPdR4FPAJGAlcCbwVw4cnrP7NrnAXcB5QA2w3MwWu/uabuuVhPt+oS+/wJGkjmsiku3S/Ur8KeANwGvu/jZgLrC75004Hahy9w3u3kbQv+GSJOt9GfgGQd+HjNIwnCKS7dJNCi3u3gJgZoXu/g9gdi/bTAQ2J0zXhPPizOxUYLK7/7anHZnZtWZWaWaV9fX1aYZ86GpjzUxQUhCRLJZuUqgJ+yk8BjxpZr8BXjucA5tZDnAHQftEj9z9Hnef5+7zKioqDuewKe1tbaexpV2PzBaRrJZuj+ZLw7e3mdkfgTLg971stgWYnDA9KZzXpQQ4AXjGzADGAYvN7GJ3r0wnriOpVrejiogc+pNO3f3ZNFddDswys2kEyeByYEHCfmJAede0mT0D3JSJhADqzSwiAn0fo7lX7t4OLASWAmuBR919tZndbmYXR3XcvuoahlNXCiKSzSIdE8HdlwBLus27NcW6b40ylt50XSmo45qIZDP10gptjbVwVHEBRfm5mQ5FRCRjlBRCdbFmtSeISNZTUgipN7OIiJJCXF2jejOLiCgpAM1tHezet1/jKIhI1lNSQLejioh0UVJAHddERLooKaBhOEVEuigpEDQyA4xTxzURyXJKCsDW3c2MGp7PsAJ1XBOR7KakQNfgOqo6EhFRUkAd10REuigpoI5rIiJdsj4ptOzvYOfeNsarkVlEREmhq4/C+JFqUxARyfqkoGE4RURel/VJoa4xeMSF2hRERCJOCmZ2gZmtM7MqM7s5yfLrzOzvZrbSzJ4zszlRxpNM15WCOq6JiESYFMwsF7gLeCcwB7giSaG/yN1PdPdTgG8Cd0QVTyp1sRZKi/IoLox0ZFIRkUEhyiuF04Eqd9/g7m3Aw8AliSu4e2PCZDHgEcaT1NbdLUxQI7OICABRfj2eCGxOmK4Bzui+kpldD9wIFADnRBhaz/qhAAAMSElEQVRPUnWNGoZTRKRLxhua3f0ud58BfA74YrJ1zOxaM6s0s8r6+vojevw69WYWEYmLMilsASYnTE8K56XyMPCuZAvc/R53n+fu8yoqKo5YgK3tHTQ0tTGuVNVHIiIQbVJYDswys2lmVgBcDixOXMHMZiVMXgSsjzCeg2xvbAVg/EhdKYiIQIRtCu7ebmYLgaVALnCvu682s9uBSndfDCw0s3OB/cAu4Kqo4klm624NwykikijS+zDdfQmwpNu8WxPefyrK4/ema3AdJQURkUDGG5ozKd5xTWMpiIgAWZ4U6mItlBTmMUId10REgCxPCrWxZjUyi4gkyPKkoGE4RUQSZX1S0OA6IiKvy9qk0NbeSUNTqx5xISKSIGuTwvY9LbjrdlQRkURZmxQ0DKeIyMGyNils1TCcIiIHydqkUBfTMJwiIt1lbVKojbVQXJBLiTquiYjEZW1SqIu1MK6sCDPLdCgiIgNG1iaF2piG4RQR6S6Lk0Iz49RxTUTkAFmZFPZ3dLJ9T6vuPBIR6SYrk0L9nlbc9chsEZHusjIp1KqPgohIUlmZFF7vzaykICKSKNKkYGYXmNk6M6sys5uTLL/RzNaY2Soze9rMjo4yni61Yce18aWqPhIRSRRZUjCzXOAu4J3AHOAKM5vTbbUVwDx3Pwn4BfDNqOJJVBtrYVh+LqXD1HFNRCRRlFcKpwNV7r7B3duAh4FLEldw9z+6+75w8nlgUoTxxNXFWhivjmsiIgeJMilMBDYnTNeE81L5CPC7ZAvM7FozqzSzyvr6+sMOrDbWrGceiYgkMSAams3s/cA84FvJlrv7Pe4+z93nVVRUHPbxgisFtSeIiHQXZaX6FmBywvSkcN4BzOxc4BbgbHdvjTAeADo6nW3quCYiklSUVwrLgVlmNs3MCoDLgcWJK5jZXOCHwMXuvj3CWOLq97TS0emqPhIRSSKypODu7cBCYCmwFnjU3Veb2e1mdnG42reAEcDPzWylmS1OsbsjJn47qpKCiMhBIr0n092XAEu6zbs14f25UR4/ma6Oa7pSEBE52IBoaO5PXcNwTlBDs4jIQbIuKdTFminMy2Hk8PxMhyIiMuBkXVKoVcc1EZGUsi4pdA3DKSIiB8u6pFCrjmsiIillVVLo6HS2NbbodlQRkRSyKinsaGqlvdOVFEREUsiqpFAb76Og6iMRkWSyMinoSkFEJLmsSgp1esSFiEiPsiop1MZaKMjNYXRxQaZDEREZkLIuKYxTxzURkZSyKimo45qISM+yKinUNjarPUFEpAdZkxQ6O51tsVb1ZhYR6UHWJIUde9to6+jUlYKISA+GfFK4+9lqllU3HDC4zrLqBu5+tjrDkYmIDDyRJgUzu8DM1plZlZndnGT5WWb2spm1m9l7o4jhpEllLFy0gj+uC4aArt/TwsJFKzhpUlkUhxMRGdQiSwpmlgvcBbwTmANcYWZzuq22CbgaWBRVHPNnlHPngrnxK4NvP/Eqdy6Yy/wZ5VEdUkRk0IrySuF0oMrdN7h7G/AwcEniCu6+0d1XAZ0RxsH8GeW8bfYYAD5wxtFKCCIiKUSZFCYCmxOma8J5h8zMrjWzSjOrrK+vP+Ttl1U38NcNO/jkOTN58MVNLKtu6EsYIiJD3qBoaHb3e9x9nrvPq6ioOKRtl1U3sHDRCu5cMJcbz5/NnQvmsnDRCiUGEZEkokwKW4DJCdOTwnn9alVN7IA2hK42hlU1sf4ORURkwMuLcN/LgVlmNo0gGVwOLIjweEldd/aMg+bNn1GudgURkSQiu1Jw93ZgIbAUWAs86u6rzex2M7sYwMzeYGY1wPuAH5rZ6qjiERGR3kV5pYC7LwGWdJt3a8L75QTVSiIiMgAMioZmERHpH0oKIiISp6QgIiJx5u6ZjuGQmFk98FofNy8HBmIHBcV1aBTXoRuosSmuQ3M4cR3t7r129Bp0SeFwmFmlu8/LdBzdKa5Do7gO3UCNTXEdmv6IS9VHIiISp6QgIiJx2ZYU7sl0ACkorkOjuA7dQI1NcR2ayOPKqjYFERHpWbZdKYiISA+UFEREJG5IJoU0xoYuNLNHwuUvmNnUfohpspn90czWmNlqM/tUknXeamYxM1sZvm5Ntq8IYttoZn8Pj1mZZLmZ2ffD87XKzE7th5hmJ5yHlWbWaGaf7rZOv50vM7vXzLab2SsJ80ab2ZNmtj78OSrFtleF66w3s6sijulbZvaP8HP6tZmNTLFtj595RLHdZmZbEj6vC1Ns2+P/bwRxPZIQ00YzW5li20jOWaqyIWN/X+4+pF5ALlANTAcKgL8Bc7qt83Hg7vD95cAj/RDXeODU8H0J8GqSuN4KPJ6Bc7YRKO9h+YXA7wADzgReyMBnWkfQ+SYj5ws4CzgVeCVh3jeBm8P3NwPfSLLdaGBD+HNU+H5UhDGdD+SF77+RLKZ0PvOIYrsNuCmNz7rH/98jHVe35d8Bbu3Pc5aqbMjU39dQvFLodWzocPr+8P0vgLebmUUZlLvXuvvL4fs9BI8T79PwpBlwCfBTDzwPjDSz8f14/LcD1e7e157sh83d/wTs7DY78e/ofuBdSTZ9B/Cku+90913Ak8AFUcXk7k948Nh6gOfJ0FOIU5yvdKTz/xtJXGEZcBnw0JE6XpoxpSobMvL3NRSTQjpjQ8fXCf+BYsBR/RIdEFZXzQVeSLL4jWb2NzP7nZkd308hOfCEmb1kZtcmWX7Extvuo8tJ/Y+aifPVZay714bv64CxSdbJ5Ln7MMEVXjK9feZRWRhWbd2bojokk+frLcA2d1+fYnnk56xb2ZCRv6+hmBQGNDMbAfwS+LS7N3Zb/DJBFcnJwP8Aj/VTWG9291OBdwLXm9lZ/XTcXplZAXAx8PMkizN1vg7iwbX8gLm/28xuAdqBB1OskonP/AfADOAUoJagqmYguYKerxIiPWc9lQ39+fc1FJNCOmNDx9cxszygDNgRdWBmlk/woT/o7r/qvtzdG929KXy/BMg3s8jHDXX3LeHP7cCvCS7hE2VyvO13Ai+7+7buCzJ1vhJs66pGC39uT7JOv587M7sa+BfgyrAwOUgan/kR5+7b3L3D3TuBH6U4Zkb+1sJy4N3AI6nWifKcpSgbMvL3NRSTQnxs6PBb5uXA4m7rLAa6WunfC/wh1T/PkRLWV/4fsNbd70ixzriutg0zO53g84k0WZlZsZmVdL0naKh8pdtqi4EPWuBMIJZwWRu1lN/eMnG+ukn8O7oK+E2SdZYC55vZqLC65PxwXiTM7ALgs8DF7r4vxTrpfOZRxJbYDnVpimOm8/8bhXOBf7h7TbKFUZ6zHsqGzPx9HemW9IHwIrhb5lWCuxhuCefdTvCPAlBEUB1RBbwITO+HmN5McPm3ClgZvi4ErgOuC9dZCKwmuOPieWB+P8Q1PTze38Jjd52vxLgMuCs8n38H5vXT51hMUMiXJczLyPkiSEy1wH6CetuPELRDPQ2sB54CRofrzgN+nLDth8O/tSrgQxHHVEVQx9z1N9Z1l90EYElPn3k/nK+fhX8/qwgKvPHdYwunD/r/jTKucP59XX9XCev2yznroWzIyN+XHnMhIiJxQ7H6SERE+khJQURE4pQUREQkTklBRETilBRERCROSUGkH1nwZNfHMx2HSCpKCiIiEqekIJKEmb3fzF4Mn53/QzPLNbMmM/tu+Mz7p82sIlz3FDN73l4fw2BUOH+mmT0VPrDvZTObEe5+hJn9woJxDx6M+gm9IodCSUGkGzM7Dvg34E3ufgrQAVxJ0MO60t2PB54FvhRu8lPgc+5+EkGP3a75DwJ3efDAvvkEPWkheArmpwmemT8deFPkv5RImvIyHYDIAPR24DRgefglfhjBw8g6ef2BaQ8AvzKzMmCkuz8bzr8f+Hn4nJyJ7v5rAHdvAQj396KHz9ixYJSvqcBz0f9aIr1TUhA5mAH3u/vnD5hp9p/d1uvrM2JaE953oP9DGUBUfSRysKeB95rZGIiPlXs0wf/Le8N1FgDPuXsM2GVmbwnnfwB41oMRtGrM7F3hPgrNbHi//hYifaBvKCLduPsaM/siwShbOQRP1Lwe2AucHi7bTtDuAMFjje8OC/0NwIfC+R8Afmhmt4f7eF8//hoifaKnpIqkycya3H1EpuMQiZKqj0REJE5XCiIiEqcrBRERiVNSEBGROCUFERGJU1IQEZE4JQUREYn7/38xBxTzlD1GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3aebbfba20>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs No of epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear from the above picture that the model probably won't cross the accuracy threshold of 90% even after training for a very long time. One possible reason for this is that the learning rate might be too high. The model's parameters may be \"bouncing\" around the optimal set of parameters for the lowest loss. You can try reducing the learning rate and training for a few more epochs to see if it helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root = 'testdata/',train = False, download = True, transform = transforms.ToTensor())\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3aebc55518>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 ,predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ae743b748>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim = 1)\n",
    "    return preds[0].item()\n",
    "\n",
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model always require batch of images as input so we input batch of size 1. img.unsqueeze simply adds another dimension at the begining of the 1x28x28 tensor, making it a 1x1x28x28 tensor, which the model views as a batch containing a single image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 ,predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3af0426eb8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[10]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9 ,predicted: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3af0393b70>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[193]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2 ,predicted: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3af032e940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[1839]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('Label:', label, ',predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.\n",
    "\n",
    "As a final step, let's also look at the overall loss and accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.6443870663642883, 'val_acc': 0.864815890789032}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle = True, batch_size = 1024)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0228,  0.0044,  0.0320,  ...,  0.0190, -0.0125,  0.0334],\n",
       "                      [ 0.0162, -0.0049, -0.0139,  ...,  0.0027,  0.0168, -0.0010],\n",
       "                      [-0.0354,  0.0040, -0.0228,  ..., -0.0122, -0.0245, -0.0249],\n",
       "                      ...,\n",
       "                      [ 0.0214,  0.0108,  0.0345,  ..., -0.0318, -0.0122, -0.0131],\n",
       "                      [-0.0127,  0.0172,  0.0006,  ...,  0.0095, -0.0199, -0.0151],\n",
       "                      [ 0.0121, -0.0302,  0.0143,  ...,  0.0132, -0.0077, -0.0318]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0570,  0.0569,  0.0022, -0.0125,  0.0010,  0.0357, -0.0333,  0.0636,\n",
       "                      -0.1027, -0.0329]))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .state_dict method returns an OrderedDict containing all the weights and bias matrices mapped to the right attributes of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0118,  0.0084, -0.0004,  ..., -0.0097, -0.0250,  0.0311],\n",
       "                      [ 0.0080,  0.0084,  0.0069,  ..., -0.0092, -0.0096,  0.0097],\n",
       "                      [-0.0278,  0.0345,  0.0055,  ..., -0.0186,  0.0141,  0.0165],\n",
       "                      ...,\n",
       "                      [-0.0252,  0.0085, -0.0052,  ..., -0.0268, -0.0118, -0.0190],\n",
       "                      [ 0.0262,  0.0207,  0.0227,  ..., -0.0231,  0.0292, -0.0260],\n",
       "                      [-0.0146, -0.0337,  0.0020,  ..., -0.0155, -0.0282, -0.0068]])),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0261,  0.0018,  0.0213, -0.0091, -0.0070,  0.0284,  0.0129,  0.0120,\n",
       "                       0.0313, -0.0132]))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MnistModel()\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3220322132110596, 'val_acc': 0.11022601276636124}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0228,  0.0044,  0.0320,  ...,  0.0190, -0.0125,  0.0334],\n",
       "                      [ 0.0162, -0.0049, -0.0139,  ...,  0.0027,  0.0168, -0.0010],\n",
       "                      [-0.0354,  0.0040, -0.0228,  ..., -0.0122, -0.0245, -0.0249],\n",
       "                      ...,\n",
       "                      [ 0.0214,  0.0108,  0.0345,  ..., -0.0318, -0.0122, -0.0131],\n",
       "                      [-0.0127,  0.0172,  0.0006,  ...,  0.0095, -0.0199, -0.0151],\n",
       "                      [ 0.0121, -0.0302,  0.0143,  ...,  0.0132, -0.0077, -0.0318]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0570,  0.0569,  0.0022, -0.0125,  0.0010,  0.0357, -0.0333,  0.0636,\n",
       "                      -0.1027, -0.0329]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.643673300743103, 'val_acc': 0.8652044534683228}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model2, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
